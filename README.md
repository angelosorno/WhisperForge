# WhisperForge Web Application ğŸ™ï¸

Una aplicaciÃ³n web profesional para transcripciÃ³n de audio con **Next.js 14** y **FastAPI**, potenciada por **OpenAI Whisper**.

## ğŸŒŸ CaracterÃ­sticas

- âœ… **Interfaz Moderna**: UI premium con Next.js 14, TypeScript y Tailwind CSS
- âœ… **Progreso en Tiempo Real**: WebSocket para actualizaciones en vivo
- âœ… **MÃºltiples Formatos**: Soporta MP3, WAV, M4A, MP4, FLAC y mÃ¡s de 30 formatos
- âœ… **Alta PrecisiÃ³n**: Modelos Whisper large-v3 para transcripciones de alta fidelidad
- âœ… **Drag & Drop**: Carga de archivos intuitiva
- âœ… **MÃ©tricas Detalladas**: RTF, duraciÃ³n, palabras, segmentos, etc.
- âœ… **ConfiguraciÃ³n Flexible**: Personaliza modelo, idioma, prompt y mÃ¡s
- âœ… **Dark Mode**: Soporte completo para tema oscuro

## ğŸ“‹ Requisitos Previos

- **Python 3.12+**
- **Node.js 18+** y npm
- **FFmpeg** (para procesamiento de audio)
- **Entorno virtual Python** (recomendado)

## ğŸš€ InstalaciÃ³n

### 1. Backend (FastAPI)

```bash
# Navegar al directorio backend
cd backend

# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Copiar y configurar variables de entorno
cp .env.example .env
# Editar .env segÃºn tus necesidades
```

### 2. Frontend (Next.js)

```bash
# Navegar al directorio frontend
cd frontend

# Instalar dependencias
npm install

# Copiar y configurar variables de entorno
cp .env.local.example .env.local
# Editar .env.local si es necesario
```

## â–¶ï¸ EjecuciÃ³n

### Iniciar Backend

```bash
cd backend
source venv/bin/activate  # Activar entorno virtual
python main.py
```

El backend estarÃ¡ disponible en: `http://localhost:8000`
- API Docs: `http://localhost:8000/docs`
- Health Check: `http://localhost:8000/api/health`

### Iniciar Frontend

```bash
cd frontend
npm run dev
```

El frontend estarÃ¡ disponible en: `http://localhost:3000`

## ğŸ“– Uso

1. **Accede a la aplicaciÃ³n** en `http://localhost:3000`
2. **Navega a "Transcribir"** desde el menÃº
3. **Arrastra un archivo de audio/video** o haz clic para seleccionar
4. **Configura las opciones** (modelo, idioma, etc.)
5. **Haz clic en "Iniciar TranscripciÃ³n"**
6. **Observa el progreso en tiempo real** en la pÃ¡gina del trabajo
7. **Descarga la transcripciÃ³n** cuando estÃ© completa

## ğŸ—ï¸ Estructura del Proyecto

```
WhisperForge/
â”œâ”€â”€ backend/                 # FastAPI application
â”‚   â”œâ”€â”€ api/                # API routes
â”‚   â”œâ”€â”€ core/               # Core logic (transcription, job manager)
â”‚   â”œâ”€â”€ models/             # Pydantic schemas
â”‚   â”œâ”€â”€ main.py             # Application entry point
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”‚
â”œâ”€â”€ frontend/               # Next.js application
â”‚   â”œâ”€â”€ app/               # App Router pages
â”‚   â”‚   â”œâ”€â”€ jobs/         # Jobs list and detail pages
â”‚   â”‚   â”œâ”€â”€ transcribe/   # Transcription page
â”‚   â”‚   â””â”€â”€ page.tsx      # Homepage
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”œâ”€â”€ lib/              # Utilities and API client
â”‚   â””â”€â”€ package.json      # Node dependencies
â”‚
â”œâ”€â”€ pending/               # Upload directory
â”œâ”€â”€ processing/            # Processing directory
â”œâ”€â”€ done/                  # Completed jobs
â””â”€â”€ failed/                # Failed jobs
```

## ğŸ”§ ConfiguraciÃ³n

### Backend (.env)

```env
# Whisper Configuration
WHISPER_MODEL=large-v3
WHISPER_LANGUAGE=es
WHISPER_TEMPERATURE=0.0
WHISPER_BEAM_SIZE=8
NORMALIZE_AUDIO=true

# Server
HOST=0.0.0.0
PORT=8000

# CORS
CORS_ORIGINS=http://localhost:3000
```

### Frontend (.env.local)

```env
NEXT_PUBLIC_API_URL=http://localhost:8000/api
NEXT_PUBLIC_WS_URL=ws://localhost:8000/api/ws
```

## ğŸ“¡ API Endpoints

### REST API

- `GET /api/health` - Health check
- `POST /api/upload` - Upload audio file
- `POST /api/transcribe/{job_id}` - Start transcription
- `GET /api/jobs` - List all jobs
- `GET /api/jobs/{job_id}` - Get job details
- `GET /api/jobs/{job_id}/download` - Download transcript
- `DELETE /api/jobs/{job_id}` - Delete job

### WebSocket

- `WS /api/ws/{job_id}` - Real-time progress updates

## ğŸ¨ TecnologÃ­as

### Backend
- **FastAPI** - Modern Python web framework
- **OpenAI Whisper** - Speech-to-text model
- **PyTorch** - ML framework
- **WebSockets** - Real-time communication
- **Pydantic** - Data validation

### Frontend
- **Next.js 14** - React framework with App Router
- **TypeScript** - Type-safe JavaScript
- **Tailwind CSS** - Utility-first CSS
- **React Query** - Data fetching and caching
- **Axios** - HTTP client
- **Lucide React** - Icon library

## ğŸ› Troubleshooting

### Backend

**Error: `ffmpeg` no encontrado**
```bash
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt-get install ffmpeg
```

**Error: MPS no soportado**
```bash
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

### Frontend

**Error: Node.js no encontrado**
```bash
# Instalar Node.js desde https://nodejs.org/
# O usar nvm:
nvm install 18
nvm use 18
```

**Error de conexiÃ³n con backend**
- Verifica que el backend estÃ© corriendo en `http://localhost:8000`
- Revisa las variables de entorno en `.env.local`

## ğŸ“ Notas

- **Modelos Whisper disponibles**: `base`, `small`, `medium`, `large-v2`, `large-v3`
- **Idiomas soportados**: 100+ idiomas (espaÃ±ol, inglÃ©s, francÃ©s, etc.)
- **TamaÃ±o mÃ¡ximo de archivo**: 500 MB (configurable)
- **Formatos soportados**: MP3, WAV, M4A, MP4, FLAC, OGG, AAC, WMA, MOV, AVI, MKV, y mÃ¡s

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“œ Licencia

MIT License - Ver archivo LICENSE para mÃ¡s detalles

---

**WhisperForge** - *Forjando transcripciones claras a partir de audios* ğŸ”¥




- Version CLI

# âš’ï¸ WhisperForge

**Forjando transcripciones claras a partir de audios**  
Un pipeline automatizado con [OpenAI Whisper](https://github.com/openai/whisper) para convertir grabaciones en texto organizado, con carpetas limpias, logs de ejecuciÃ³n y mÃ©tricas de trazabilidad.  
DiseÃ±ado para la transcripciÃ³n automÃ¡tica de audio a texto con alta fidelidad. Soporta mÃºltiples formatos (MP3, WAV, M4A, MP4, FLAC, CAF, AIFF, MOV, entre otros).

### Casos de uso:
- ğŸ™ï¸ TranscripciÃ³n de **podcasts** y **entrevistas** para publicaciÃ³n en blogs o medios digitales.  
- ğŸ“š GeneraciÃ³n de **notas de conferencias, clases o cursos en lÃ­nea**.  
- ğŸ§ Procesamiento de **grabaciones personales** (dictados, ideas, reuniones).  
- ğŸ“° CreaciÃ³n de **subtÃ­tulos o documentaciÃ³n** a partir de archivos de audio y video.  

> Con soporte para **espaÃ±ol e idiomas internacionales**, WhisperForge ofrece un flujo simple: coloca un archivo en la carpeta `pending/` y el sistema lo transforma en texto preciso, organizado y listo para usar.  

---

## ğŸ“¥ Clona el repositorio desde GitHub

```bash
git clone git@github.com:angelosorno/WhisperForge.git
cd WhisperForge
```

---

## ğŸ“¦ InstalaciÃ³n rÃ¡pida (Recomendada)

Ejecuta el instalador automÃ¡tico (macOS / Linux):

```bash
chmod +x Install.sh && ./Install.sh
```

El script harÃ¡ lo siguiente por ti:

- Crea un **entorno virtual** `env/`.  
- Instala **FFmpeg** (Homebrew en macOS, APT en Debian/Ubuntu).  
- Instala **PyTorch** segÃºn tu sistema (Apple Silicon, NVIDIA GPU o CPU).  
- Instala **Whisper** desde el repo oficial.  
- Crea la estructura de carpetas: `pending/`, `processing/`, `done/`, `failed/`.  

> En **Windows**, usa WSL2 o instala manualmente (ver secciÃ³n â€œInstalaciÃ³n manualâ€).  

---

## ğŸ“– DescripciÃ³n

**WhisperForge** es un sistema local en Python que organiza y transcribe automÃ¡ticamente archivos de audio.  
Deja un archivo en **`pending/`**, y el sistema lo **forja** ğŸ”¥ en texto con Whisper, depositÃ¡ndolo en **`done/`** con su `.txt` y un `meta.json` con mÃ©tricas de ejecuciÃ³n.  

ğŸ‘‰ El cuaderno principal para ejecutar el sistema es:  

**`WhisperLoop.ipynb`**  

TambiÃ©n se incluye:  
- **`WhisperBase.ipynb`** â†’ versiÃ³n bÃ¡sica para transcribir un solo audio rÃ¡pidamente.  

---

## ğŸ“‚ Estructura de carpetas

```
WhisperForge/
â”‚
â”œâ”€â”€ pending/       # AquÃ­ dejas los audios sin procesar
â”œâ”€â”€ processing/    # Usado internamente durante el trabajo
â”œâ”€â”€ done/          # Resultados finales (audio + txt + meta.json)
â”œâ”€â”€ failed/        # Jobs con error (contienen audio + error.log)
â”‚
â”œâ”€â”€ pipeline.log   # Log global con todas las ejecuciones
â”œâ”€â”€ Install.sh     # Instalador automÃ¡tico
â”œâ”€â”€ requirements.txt # Dependencias
â”œâ”€â”€ WhisperLoop.ipynb # Cuaderno principal
â””â”€â”€ WhisperBase.ipynb # Cuaderno bÃ¡sico
```

---

## â–¶ï¸ Uso bÃ¡sico

1. Activa el entorno:
   ```bash
   source env/bin/activate        # macOS / Linux
   ```

2. Coloca los audios en **`pending/`**. Formatos aceptados:
   ```
   .m4a .wav .mp3 .flac .ogg .aac .wma .mkv .mp4 .caf .aiff .aif .mov
   ```

3. Ejecuta el cuaderno principal:
   ```bash
   jupyter notebook WhisperLoop.ipynb
   ```

4. Cada audio genera en **`done/`**:
   - ğŸµ Audio original  
   - ğŸ“ `<timestamp>-<model>-<slug>.txt`  
   - ğŸ“‘ `meta.json` con mÃ©tricas  

Si algo falla:  
- El audio se **mantiene en `pending/`** o  
- Se mueve a **`failed/`** con su `error.log` para diagnÃ³stico.  

---

## âš™ï¸ ConfiguraciÃ³n recomendada

En tu `WhisperLoop.ipynb`, puedes configurar el modelo, idioma y prompt:

```python
MODEL_NAME = "large-v3"
LANG = "es"
INITIAL_PROMPT = (
    "TranscripciÃ³n fiel en espaÃ±ol de un archivo de audio. "
    "Usar ortografÃ­a y gramÃ¡tica correctas, con buena puntuaciÃ³n. "
    "Contexto: discurso, conferencia o grabaciÃ³n personal. "
    "Palabras clave: claridad, precisiÃ³n, coherencia, fidelidad."
)
```

AdemÃ¡s:  
- `NORMALIZE_AUDIO = True` â†’ Normaliza a **WAV 16kHz mono (PCM16)** antes de transcribir.  
- `NORMALIZE_AUDIO = False` â†’ Usa directamente el archivo original.  

---

## ğŸ“‹ InstalaciÃ³n con requirements.txt

```bash
pip install -r requirements.txt
```

Incluye:

```
torch
torchaudio
torchvision
git+https://github.com/openai/whisper.git
```

---

## ğŸ“Š MÃ©tricas y trazabilidad

Cada job guarda un `meta.json` con informaciÃ³n como:  

- â±ï¸ `elapsed_sec` â†’ tiempo total de ejecuciÃ³n.  
- ğŸµ `audio_duration_sec` â†’ duraciÃ³n real del audio procesado.  
- âš¡ `rtf` â†’ Real Time Factor (relaciÃ³n entre tiempo de ejecuciÃ³n y duraciÃ³n del audio).  
- ğŸ“ˆ `coverage_ratio` â†’ hasta dÃ³nde llegÃ³ la transcripciÃ³n respecto al audio.  
- ğŸ”§ ConfiguraciÃ³n usada: modelo, device, beam_size, temperature, prompt.  
- âœ… Si el audio fue **normalizado** o no.  
- ğŸ”‘ Hash SHA1 de los archivos para verificaciÃ³n de integridad.  

Al final de cada ejecuciÃ³n, el sistema imprime un **informe en consola** con:  
- NÃºmero total de jobs.  
- Exitosos / fallidos.  
- DuraciÃ³n promedio de audios.  
- Tiempo promedio de ejecuciÃ³n.  
- RTF promedio.  

---

## ğŸ§ª InstalaciÃ³n manual (Si no usas el script)

```bash
python3 -m venv env
source env/bin/activate
pip install -U pip setuptools wheel
pip install torch torchvision torchaudio
pip install git+https://github.com/openai/whisper.git
brew install ffmpeg   # macOS
sudo apt-get -y install ffmpeg   # Linux
```

> Ajusta PyTorch segÃºn tu hardware (CUDA/MPS/CPU).  

---

## ğŸ§¯ Troubleshooting rÃ¡pido

- **MPS (Apple Silicon) falla con operator no soportado**  
  ```bash
  export PYTORCH_ENABLE_MPS_FALLBACK=1
  ```  
- **`ffmpeg` no encontrado** â†’ instala con Homebrew o APT.  
- **Memoria insuficiente con `large-v3`** â†’ usa `medium` o procesa audios mÃ¡s cortos.  
- **Archivos con puntos/espacios en el nombre** â†’ el sistema los normaliza automÃ¡ticamente (slugify).  

---

## ğŸ¤ Contribuir

1. Haz un fork ğŸ´  
2. Crea una rama  
3. EnvÃ­a un PR  

Issues bienvenidos: rendimiento, compatibilidad, documentaciÃ³n.  

---

## ğŸ“œ Licencia

**MIT**. Usa y adapta libremente.  

---

> **WhisperForge**: *â€œConvierte cualquier audio en texto claro, con mÃ©tricas y control total.â€*  
