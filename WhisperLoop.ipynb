{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f595d4",
   "metadata": {},
   "source": [
    "Model Selector\n",
    "> turbo, base, small, medium, large, large-v2, turbo, large-v3-turbo, large-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ca4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417f82c",
   "metadata": {},
   "source": [
    "Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cd2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilita fallback CPU para ops no soportadas\n",
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "import re, unicodedata, shutil, datetime, traceback, time\n",
    "import platform, torch, whisper\n",
    "import subprocess, json, hashlib\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from whisper.tokenizer import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcbee8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS built: True\n"
     ]
    }
   ],
   "source": [
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", getattr(torch.backends.mps, \"is_built\", lambda: None)())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f30f4a",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1257ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = \"cpu\"\n",
    "# FP16 = False  # en CPU no uses fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56aeeb",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10697ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# FP16 = (DEVICE == \"cuda\")  # en GPU con CUDA sí conviene usar fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69c529",
   "metadata": {},
   "source": [
    "Apple Silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ea3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = \"mps\"\n",
    "#     FP16 = False\n",
    "# else:\n",
    "#     DEVICE = \"cpu\"\n",
    "#     FP16 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51b33c",
   "metadata": {},
   "source": [
    "Detectar Hardware: CUDA > MPS > CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82a380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOW_MPS = False  # pon True si quieres probar MPS\n",
    "\n",
    "def select_candidates():\n",
    "    cands = []\n",
    "    if torch.cuda.is_available():\n",
    "        cands.append((\"cuda\", True))       # CUDA con fp16=True\n",
    "    if ALLOW_MPS and getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        cands.append((\"mps\", False))       # MPS con fp16=False (más estable)\n",
    "    cands.append((\"cpu\", False))           # CPU con fp16=False\n",
    "    return cands\n",
    "\n",
    "def load_whisper_with_backoff(model_name: str):\n",
    "    logger = globals().get(\"log_line\", print)\n",
    "    last_err = None\n",
    "    for dev, fp16 in select_candidates():\n",
    "        try:\n",
    "            logger(f\"Cargando Whisper '{model_name}' en '{dev}' fp16={fp16}…\")\n",
    "            model = whisper.load_model(model_name, device=dev)\n",
    "            extra = \"\"\n",
    "            if dev == \"cuda\":\n",
    "                try:\n",
    "                    extra = f\" | GPU: {torch.cuda.get_device_name(0)} cap={torch.cuda.get_device_capability(0)}\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "            logger(f\"[HW] device={dev} fp16={fp16} torch={torch.__version__} os={platform.system()} {platform.release()}{extra}\")\n",
    "            return model, dev, fp16\n",
    "        except (NotImplementedError, RuntimeError) as e:\n",
    "            msg = str(e)\n",
    "            head = msg.splitlines()[0] if msg else e.__class__.__name__\n",
    "            # MPS no soporta cierto operador\n",
    "            if (\"MPS\" in msg) or (\"SparseMPS\" in msg) or (\"aten::_sparse_coo_tensor_with_dims_and_tensors\" in msg):\n",
    "                logger(f\"[WARN] Backend {dev} falló: {head}. Probando siguiente opción…\")\n",
    "                last_err = e\n",
    "                continue\n",
    "            # CUDA sin inicializar / sin GPU\n",
    "            if (\"CUDA error\" in msg) or (\"no CUDA GPUs are available\" in msg) or (\"CUDA initialization\" in msg):\n",
    "                logger(f\"[WARN] Backend {dev} falló: {head}. Probando siguiente opción…\")\n",
    "                last_err = e\n",
    "                continue\n",
    "            raise\n",
    "    raise last_err if last_err else RuntimeError(\"No fue posible cargar el modelo en ningún backend\")\n",
    "\n",
    "@lru_cache(maxsize=3)  # Hasta 3 modelos distintos por sesión\n",
    "def get_whisper_model(model_name: str):\n",
    "    return load_whisper_with_backoff(model_name)  # -> (model, device, fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8f36e",
   "metadata": {},
   "source": [
    "Config del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR = Path.cwd()\n",
    "PENDING     = WORKDIR / \"pending\"\n",
    "PROCESSING  = WORKDIR / \"processing\"\n",
    "DONE        = WORKDIR / \"done\"\n",
    "FAILED      = WORKDIR / \"failed\"\n",
    "LOGFILE     = WORKDIR / \"pipeline.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd72fca",
   "metadata": {},
   "source": [
    "Preparar Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d9a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (PENDING, PROCESSING, DONE, FAILED):\n",
    "    d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e719f",
   "metadata": {},
   "source": [
    "Helper Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "634d4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"long\" = YYYYMMDD-HHMMSS-name, \"short\" = MMDDHHMM-name\n",
    "NAME_STYLE = \"long\"\n",
    "LANG       = \"es\"\n",
    "# Normalización de audio (WAV 16 kHz mono PCM16)\n",
    "TEMPERATURE = 0.0\n",
    "BEAM_SIZE   = 8\n",
    "# False si quieres desactivarlo y procesar los archivos tal cual sin normalizar\n",
    "NORMALIZE_AUDIO = True  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d45ddf",
   "metadata": {},
   "source": [
    "Extensiones de audio permitidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a13d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_EXTS = {\".wav\", \".m4a\", \".mp3\", \".flac\", \".ogg\", \".oga\", \".ogx\", \".opus\", \".aac\", \".wma\", \".caf\", \".aiff\", \".aif\", \".aifc\", \".amr\", \".alaw\", \".ulaw\", \".ac3\", \".eac3\", \".dts\", \".mp4\", \".m4v\", \".mov\", \".mkv\", \".mka\", \".webm\", \".weba\", \".avi\", \".3gp\", \".3g2\", \".flv\", \".ts\", \".mp2\", \".mp1\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46690cff",
   "metadata": {},
   "source": [
    "Tokenizer multilingüe de Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5d8e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = get_tokenizer(multilingual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8da8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "StyleSample = (\n",
    "    \"Hermanos, continuamos con la enseñanza de manera clara y pausada. \"\n",
    "    \"Leemos la Escritura según la Reina-Valera 1960 y citamos así: [Juan 3:16]. \"\n",
    "    \"Gloria a Dios, seguimos con la explicación de la doctrina sin alterar el sentido de las palabras.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38bc3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glossary = [\n",
    "    \"Iglesia de Dios Ministerial de Jesucristo Internacional\",\n",
    "    \"Hna. María Luisa\",\n",
    "    \"Espíritu Santo\",\n",
    "    \"Señor Jesucristo\",\n",
    "    \"Reina-Valera 1960\",\n",
    "    \"Antiguo Testamento\",\n",
    "    \"Nuevo Testamento\",\n",
    "    \"Biblia\",\n",
    "    \"Doctrina\",\n",
    "    \"Enseñanza\",\n",
    "    \"Don de la Profecía\",\n",
    "    \"Gloria a Dios\",\n",
    "    \"Hechos 2:17\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc75afa",
   "metadata": {},
   "source": [
    "Funciones utilitarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d8ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfc(s: str) -> str: return unicodedata.normalize(\"NFC\", s)\n",
    "def slugify(filename: str) -> str:\n",
    "    base = Path(filename).stem\n",
    "    base = unicodedata.normalize(\"NFC\", base).strip().casefold()\n",
    "    base = re.sub(r\"[.\\s]+\", \"-\", base)\n",
    "    base = re.sub(r\"[^\\w\\-.]+\", \"-\", base)\n",
    "    base = re.sub(r\"-{2,}\", \"-\", base).strip(\"-\")\n",
    "    return base or \"audio\"\n",
    "def ts_long() -> str: return datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "def ts_short() -> str: return datetime.datetime.now().strftime(\"%m%d%H%M\")\n",
    "def make_job_name(audio_path: Path) -> str:\n",
    "    ts = ts_long() if NAME_STYLE == \"long\" else ts_short()\n",
    "    model_tag = slugify(MODEL_NAME)\n",
    "    return f\"{ts}-{model_tag}-{slugify(audio_path.name)}\"\n",
    "def list_audios(pending_dir: Path):\n",
    "    def has_allowed_ext(p: Path) -> bool:\n",
    "        return any(ext.lower() in AUDIO_EXTS for ext in p.suffixes)\n",
    "    files = []\n",
    "    for p in pending_dir.iterdir():\n",
    "        if p.is_file():\n",
    "            if has_allowed_ext(p):\n",
    "                files.append(p)\n",
    "            else:\n",
    "                log_line(f\"[SKIP] {p.name} (Extensión no permitida)\")\n",
    "    return sorted(files, key=lambda p: p.stat().st_ctime)\n",
    "def log_line(msg: str):\n",
    "    LOGFILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with LOGFILE.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.datetime.now().isoformat()}  {msg}\\n\")\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc35b897",
   "metadata": {},
   "source": [
    "Construcción del Prompt Inicial para Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5754f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROMPT] tokens=159\n"
     ]
    }
   ],
   "source": [
    "def BuildGlossarySentence(terms: list[str]) -> str:\n",
    "    terms = [t.strip() for t in terms if t and t.strip()]\n",
    "    return (\" En esta charla se mencionan: \" + \", \".join(terms) + \".\") if terms else \"\"\n",
    "\n",
    "def TrimTo224Tokens(text: str) -> str:\n",
    "    toks = Tokenizer.encode(text)\n",
    "    toks = toks[-224:]  # Whisper solo atiende a los ÚLTIMOS 224 tokens\n",
    "    return Tokenizer.decode(toks)\n",
    "\n",
    "def BuildPromptForWhisper(styleSample: str, glossary: list[str]) -> str:\n",
    "    prompt = (styleSample.strip() + BuildGlossarySentence(glossary)).strip()\n",
    "    return TrimTo224Tokens(prompt)\n",
    "# ÚNICA fuente de verdad del prompt\n",
    "INITIAL_PROMPT = BuildPromptForWhisper(StyleSample, Glossary)\n",
    "\n",
    "# Reporta cuántos tokens quedaron\n",
    "try:\n",
    "    prompt_tokens = len(Tokenizer.encode(INITIAL_PROMPT))\n",
    "    log_line(f\"[PROMPT] tokens={prompt_tokens}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a51455",
   "metadata": {},
   "source": [
    "RTF (Real-Time Factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb68102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffprobe_duration(path: str | Path) -> float:\n",
    "    \"\"\"Duración en segundos usando ffprobe (preciso y rápido).\"\"\"\n",
    "    out = subprocess.check_output([\n",
    "        \"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
    "        \"-of\", \"default=noprint_wrappers=1:nokey=1\", str(path)\n",
    "    ], text=True).strip()\n",
    "    try:\n",
    "        return float(out)\n",
    "    except:\n",
    "        return 0.0\n",
    "def sha1_file(path: Path) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b554a",
   "metadata": {},
   "source": [
    "Pasa de Segundos a Minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "594c5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_min(x: float | None) -> float | None:\n",
    "    return round(x / 60.0, 2) if (x is not None) else None\n",
    "\n",
    "def fmt_hms(seconds: float | None) -> str | None:\n",
    "    if seconds is None:\n",
    "        return None\n",
    "    seconds = int(round(seconds))\n",
    "    h, rem = divmod(seconds, 3600)\n",
    "    m, s = divmod(rem, 60)\n",
    "    if h > 0:\n",
    "        return f\"{h:d}:{m:02d}:{s:02d}\"\n",
    "    return f\"{m:d}:{s:02d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aff1bc",
   "metadata": {},
   "source": [
    "Normalización de audio (WAV 16 kHz mono PCM16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09605bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_wav_16k(in_path: Path, out_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Convierte cualquier entrada (audio o video) a WAV PCM 16 kHz mono.\n",
    "    -vn fuerza a ignorar video; aresample=soxr mejora la calidad del remuestreo.\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\", \"-i\", str(in_path),\n",
    "        \"-vn\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-af\", \"aresample=resampler=soxr:precision=33\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-c:a\", \"pcm_s16le\",\n",
    "        str(out_path)\n",
    "    ]\n",
    "    # Capturamos stdout/stderr para que el notebook no se llene de logs de ffmpeg\n",
    "    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return out_path\n",
    "def prepareAudioforWhisper(src_path: Path, job_dir: Path, enabled: bool) -> Path:\n",
    "    \"\"\"\n",
    "    Si enabled=True → genera job_dir/'input.16k.wav' y lo devuelve.\n",
    "    Si enabled=False → devuelve el path original sin tocar.\n",
    "    \"\"\"\n",
    "    if not enabled:\n",
    "        return src_path\n",
    "    target = job_dir / \"input.16k.wav\"\n",
    "    return to_wav_16k(src_path, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae853941",
   "metadata": {},
   "source": [
    "Carga del Modelo (Cacheada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47291769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Whisper 'base' en 'cpu' fp16=False…\n",
      "[HW] device=cpu fp16=False torch=2.8.0 os=Darwin 25.0.0\n",
      "Modelo 'base' listo en cpu fp16=False\n"
     ]
    }
   ],
   "source": [
    "model, DEVICE, FP16 = get_whisper_model(MODEL_NAME)\n",
    "print(f\"Modelo '{MODEL_NAME}' listo en {DEVICE} fp16={FP16}\")\n",
    "# log_line(f\"[MODEL] Modelo '{MODEL_NAME}' cargado en {DEVICE} fp16={FP16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76a9af",
   "metadata": {},
   "source": [
    "Bucle Procesador de Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "320891ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUDIO] Normalizado a 16k WAV: input.16k.wav\n",
      "[START] audio.mp3 -> 20250918-005516-base-audio [model=base, device=cpu, fp16=False]\n",
      "[PROMPT] tokens=159 preview='Hermanos, continuamos con la enseñanza de manera clara y pausada. Leemos la Escritura según la Reina-Valera 1960 y citam'\n",
      "[DONE]  audio.mp3 -> /Users/angelosorno/Documents/VSCode/IDMJI/Whisper/done/20250918-005516-base-audio (dur=0:43 ≈ 0.72 min, audio=4:24 ≈ 4.41 min, RTF=0.164)\n",
      "\n",
      "===== 📊 Informe de Ejecución =====\n",
      "Total procesados: 1\n",
      "  ✅ Exitosos: 1\n",
      "  ❌ Fallidos: 0\n",
      "Normalizados: 1/1 (100.0%)\n",
      "Duración promedio de audio: 264.4 s\n",
      "Tiempo promedio de ejecución: 0:00:43.429286\n",
      "RTF Promedio: 0.164\n",
      "===================================\n",
      "\n",
      "Terminado. Revisa ./done para los finalizados y ./pending para los no procesados.\n"
     ]
    }
   ],
   "source": [
    "audios = list_audios(PENDING)\n",
    "if not audios:\n",
    "    print(\"No hay audios en la carpeta ./pending.\")\n",
    "else:\n",
    "    total_jobs = 0\n",
    "    ok_jobs = 0\n",
    "    failed_jobs = 0\n",
    "    total_elapsed = datetime.timedelta()\n",
    "    total_audio_dur = 0.0\n",
    "    normalized_count = 0\n",
    "    rtfs = []\n",
    "\n",
    "    for audio_in in audios:\n",
    "        job_name = make_job_name(audio_in)\n",
    "        job_dir = PROCESSING / job_name\n",
    "        job_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        audio_tmp = job_dir / audio_in.name\n",
    "        shutil.copy2(str(audio_in), str(audio_tmp))\n",
    "\n",
    "        start_wall = datetime.datetime.now()\n",
    "        start_perf = time.perf_counter()\n",
    "\n",
    "        # Preparación con fallback\n",
    "        try:\n",
    "            src_for_whisper = prepareAudioforWhisper(audio_tmp, job_dir, enabled=NORMALIZE_AUDIO)\n",
    "            if src_for_whisper != audio_tmp:\n",
    "                log_line(f\"[AUDIO] Normalizado a 16k WAV: {src_for_whisper.name}\")\n",
    "                normalized_count += 1\n",
    "            else:\n",
    "                log_line(f\"[AUDIO] Sin normalizar (usando original): {audio_tmp.name}\")\n",
    "        except Exception as prep_err:\n",
    "            src_for_whisper = audio_tmp\n",
    "            log_line(f\"[AUDIO] Preparación falló, uso original: {audio_tmp.name} :: {prep_err}\")\n",
    "\n",
    "        log_line(f\"[START] {audio_in.name} -> {job_dir.name} [model={MODEL_NAME}, device={DEVICE}, fp16={FP16}]\")\n",
    "\n",
    "        try:\n",
    "            # Transcripción\n",
    "            result = model.transcribe(\n",
    "                str(src_for_whisper),\n",
    "                language=LANG,\n",
    "                task=\"transcribe\",\n",
    "                temperature=TEMPERATURE,\n",
    "                beam_size=BEAM_SIZE,\n",
    "                patience=1.0,\n",
    "                condition_on_previous_text=True,\n",
    "                initial_prompt=INITIAL_PROMPT if INITIAL_PROMPT.strip() else None,\n",
    "                fp16=FP16\n",
    "            )\n",
    "\n",
    "            # Texto\n",
    "            text = nfc(result.get(\"text\", \"\")).strip()\n",
    "            text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "            text = re.sub(r\"\\s+\\n\", \"\\n\", text).strip() + \"\\n\"\n",
    "\n",
    "            out_txt = job_dir / f\"{job_name}.txt\"\n",
    "            out_txt.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "            # Métricas\n",
    "            end_wall = datetime.datetime.now()\n",
    "            elapsed = datetime.timedelta(seconds=(time.perf_counter() - start_perf))\n",
    "\n",
    "            audio_used_path = Path(src_for_whisper)\n",
    "            try:\n",
    "                audio_duration = ffprobe_duration(audio_used_path)\n",
    "            except Exception as e:\n",
    "                audio_duration = None\n",
    "                log_line(f\"[WARN] ffprobe falló para {audio_used_path.name}: {e}\")\n",
    "\n",
    "            rtf = (elapsed.total_seconds() / audio_duration) if (audio_duration and audio_duration > 0) else None\n",
    "\n",
    "            last_end = None\n",
    "            try:\n",
    "                segs = result.get(\"segments\")\n",
    "                if isinstance(segs, list) and segs:\n",
    "                    last_end = float(segs[-1].get(\"end\", 0.0))\n",
    "            except Exception:\n",
    "                last_end = None\n",
    "\n",
    "            coverage_ratio = (last_end / audio_duration) if (last_end is not None and audio_duration and audio_duration > 0) else None\n",
    "\n",
    "            elapsed_sec_val = elapsed.total_seconds()\n",
    "            audio_dur_sec_val = audio_duration if (audio_duration and audio_duration > 0) else None\n",
    "\n",
    "            elapsed_min = sec_to_min(elapsed_sec_val)\n",
    "            audio_duration_min = sec_to_min(audio_dur_sec_val)\n",
    "            elapsed_hms = fmt_hms(elapsed_sec_val)\n",
    "            audio_duration_hms = fmt_hms(audio_dur_sec_val)\n",
    "\n",
    "            try:\n",
    "                prompt_tokens = len(Tokenizer.encode(INITIAL_PROMPT))\n",
    "                log_line(f\"[PROMPT] tokens={prompt_tokens} preview={INITIAL_PROMPT[:120]!r}\")\n",
    "            except Exception:\n",
    "                prompt_tokens = None\n",
    "            # Metadatos\n",
    "            meta = {\n",
    "                \"job_name\": job_name,\n",
    "                \"start_time\": start_wall.isoformat(),\n",
    "                \"end_time\": end_wall.isoformat(),\n",
    "                \"elapsed_sec\": round(elapsed_sec_val, 3),\n",
    "                \"audio_duration_sec\": round(audio_dur_sec_val, 3) if audio_dur_sec_val is not None else None,\n",
    "                \"elapsed_min\": elapsed_min,\n",
    "                \"audio_duration_min\": audio_duration_min,\n",
    "                \"elapsed_hms\": elapsed_hms,\n",
    "                \"audio_duration_hms\": audio_duration_hms,\n",
    "                \"rtf\": round(rtf, 3) if rtf is not None else None,\n",
    "                \"coverage_last_segment_end_sec\": round(last_end, 3) if last_end is not None else None,\n",
    "                \"coverage_ratio\": round(coverage_ratio, 4) if coverage_ratio is not None else None,\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"device\": DEVICE,\n",
    "                \"fp16\": FP16,\n",
    "                \"language\": LANG,\n",
    "                \"beam_size\": BEAM_SIZE,\n",
    "                \"temperature\": (list(TEMPERATURE) if isinstance(TEMPERATURE, tuple) else TEMPERATURE),\n",
    "                \"initial_prompt_len_chars\": len(INITIAL_PROMPT.strip()),\n",
    "                \"initial_prompt_len_tokens\": prompt_tokens,\n",
    "                \"normalized_16k\": bool(NORMALIZE_AUDIO and audio_used_path.name.endswith(\"input.16k.wav\")),\n",
    "                \"input_original_name\": audio_in.name,\n",
    "                \"input_original_sha1\": sha1_file(audio_in),\n",
    "                \"input_used_name\": audio_used_path.name,\n",
    "                \"input_used_sha1\": sha1_file(audio_used_path),\n",
    "                \"output_txt\": out_txt.name,\n",
    "                \"chars\": len(text),\n",
    "                \"words\": len(text.split()),\n",
    "                \"segments\": (len(result.get(\"segments\", [])) if isinstance(result.get(\"segments\"), list) else None),\n",
    "                \"whisper_version\": getattr(whisper, \"__version__\", \"git\"),\n",
    "                \"torch_version\": torch.__version__,\n",
    "                \"os\": f\"{platform.system()} {platform.release()}\",\n",
    "            }\n",
    "\n",
    "            # meta.json atómico\n",
    "            tmp_meta = job_dir / \"meta.json.tmp\"\n",
    "            tmp_meta.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "            tmp_meta.replace(job_dir / \"meta.json\")\n",
    "\n",
    "            # Mover a done\n",
    "            final_dir = DONE / job_name\n",
    "            shutil.move(str(job_dir), str(final_dir))\n",
    "            audio_in.unlink(missing_ok=False)\n",
    "\n",
    "            log_line(f\"[DONE]  {audio_in.name} -> {final_dir} \"\n",
    "                     f\"(dur={elapsed_hms} ≈ {elapsed_min} min, \"\n",
    "                     f\"audio={audio_duration_hms} ≈ {audio_duration_min} min, \"\n",
    "                     f\"RTF={meta['rtf']})\")\n",
    "\n",
    "            total_jobs += 1\n",
    "            ok_jobs += 1\n",
    "            total_elapsed += elapsed\n",
    "            if audio_dur_sec_val is not None:\n",
    "                total_audio_dur += audio_dur_sec_val\n",
    "            if rtf is not None:\n",
    "                rtfs.append(rtf)\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_jobs += 1\n",
    "            total_jobs += 1\n",
    "            tb = traceback.format_exc()\n",
    "            log_line(f\"[FAIL]  {audio_in.name} :: {e}\")\n",
    "            (job_dir / \"error.log\").write_text(tb, encoding=\"utf-8\")\n",
    "            failed_dir = (WORKDIR / \"failed\" / job_name)\n",
    "            failed_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(str(job_dir), str(failed_dir))\n",
    "            continue\n",
    "\n",
    "    # Informe\n",
    "    print(\"\\n===== 📊 Informe de Ejecución =====\")\n",
    "    print(f\"Total procesados: {total_jobs}\")\n",
    "    print(f\"  ✅ Exitosos: {ok_jobs}\")\n",
    "    print(f\"  ❌ Fallidos: {failed_jobs}\")\n",
    "    if total_jobs > 0:\n",
    "        print(f\"Normalizados: {normalized_count}/{total_jobs} ({(100.0*normalized_count/total_jobs):.1f}%)\")\n",
    "    if ok_jobs > 0:\n",
    "        if total_audio_dur > 0:\n",
    "            avg_audio = total_audio_dur / ok_jobs\n",
    "            print(f\"Duración promedio de audio: {avg_audio:.1f} s\")\n",
    "        else:\n",
    "            print(\"Duración promedio de audio: N/A\")\n",
    "        avg_elapsed = total_elapsed / ok_jobs\n",
    "        print(f\"Tiempo promedio de ejecución: {avg_elapsed}\")\n",
    "        if rtfs:\n",
    "            avg_rtf = sum(rtfs) / len(rtfs)\n",
    "            print(f\"RTF Promedio: {avg_rtf:.3f}\")\n",
    "        else:\n",
    "            print(\"RTF Promedio: N/A\")\n",
    "    else:\n",
    "        print(\"No hubo jobs exitosos.\")\n",
    "    print(\"===================================\")\n",
    "    print(\"\\nTerminado. Revisa ./done para los finalizados y ./pending para los no procesados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
