{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f595d4",
   "metadata": {},
   "source": [
    "Model Selector\n",
    "> turbo, base, small, medium, large, large-v2, large-v3-turbo, large-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5ca4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417f82c",
   "metadata": {},
   "source": [
    "Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90cd2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilita fallback CPU para ops no soportadas\n",
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "import re, unicodedata, shutil, datetime, traceback, time\n",
    "import platform, torch, whisper\n",
    "import subprocess, json, hashlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcbee8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS built: True\n"
     ]
    }
   ],
   "source": [
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", getattr(torch.backends.mps, \"is_built\", lambda: None)())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f30f4a",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c1257ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = \"cpu\"\n",
    "# FP16 = False  # en CPU no uses fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56aeeb",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10697ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# FP16 = (DEVICE == \"cuda\")  # en GPU con CUDA sí conviene usar fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69c529",
   "metadata": {},
   "source": [
    "Apple Silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87ea3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = \"mps\"\n",
    "#     FP16 = False\n",
    "# else:\n",
    "#     DEVICE = \"cpu\"\n",
    "#     FP16 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51b33c",
   "metadata": {},
   "source": [
    "Detectar Hardware: CUDA > MPS > CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b82a380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Whisper 'base' en 'mps' fp16=False…\n",
      "[WARN] Backend mps falló (NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseMPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_sparse_coo_tensor_with_dims_and_tensors' is only available for these backends: [MPS, Meta, SparseCPU, SparseMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "MPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:79 [backend fallback]\n",
      "Meta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta_0.cpp:13978 [kernel]\n",
      "SparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU_0.cpp:2598 [kernel]\n",
      "SparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta_0.cpp:342 [kernel]\n",
      "BackendSelect: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:792 [kernel]\n",
      "Python: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\n",
      "Functionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:375 [backend fallback]\n",
      "Named: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\n",
      "ZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\n",
      "AutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMAIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "Tracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17887 [kernel]\n",
      "AutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\n",
      "AutocastMTIA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\n",
      "AutocastMAIA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\n",
      "AutocastXPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:542 [backend fallback]\n",
      "AutocastMPS: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\n",
      "FuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\n",
      "BatchedNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\n",
      "Batched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\n",
      "PythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\n",
      "PreDispatch: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\n",
      "PythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
      "). Probando siguiente opción…\n",
      "Cargando Whisper 'base' en 'cpu' fp16=False…\n",
      "[HW] device=cpu fp16=False torch=2.8.0 os=Darwin 25.0.0\n"
     ]
    }
   ],
   "source": [
    "def select_candidates():\n",
    "    cands = []\n",
    "    if torch.cuda.is_available():\n",
    "        cands.append((\"cuda\", True))     # CUDA con fp16\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        cands.append((\"mps\", False))     # MPS con fp16=False\n",
    "    cands.append((\"cpu\", False))         # CPU\n",
    "    return cands\n",
    "\n",
    "def load_whisper_with_backoff(model_name: str):\n",
    "    last_err = None\n",
    "    for dev, fp16 in select_candidates():\n",
    "        try:\n",
    "            log_line(f\"Cargando Whisper '{model_name}' en '{dev}' fp16={fp16}…\")\n",
    "            model = whisper.load_model(model_name, device=dev)\n",
    "            return model, dev, fp16\n",
    "        except (NotImplementedError, RuntimeError) as e:\n",
    "            msg = str(e)\n",
    "            # Si es un fallo típico de MPS (SparseMPS / operador no soportado), probamos el siguiente device\n",
    "            if \"MPS\" in msg or \"SparseMPS\" in msg or \"aten::_sparse_coo_tensor_with_dims_and_tensors\" in msg:\n",
    "                log_line(f\"[WARN] Backend {dev} falló ({e.__class__.__name__}: {e}). Probando siguiente opción…\")\n",
    "                last_err = e\n",
    "                continue\n",
    "            # Otros errores: propaga\n",
    "            raise\n",
    "    # Si ninguno funcionó, re-lanza el último para que quede trazado\n",
    "    raise last_err if last_err else RuntimeError(\"No fue posible cargar el modelo en ningún backend\")\n",
    "\n",
    "# Usa esta llamada en vez de whisper.load_model(...)\n",
    "model, DEVICE, FP16 = load_whisper_with_backoff(MODEL_NAME)\n",
    "\n",
    "# (Opcional) imprime el contexto detectado\n",
    "try:\n",
    "    extra = \"\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        extra = f\" | GPU: {torch.cuda.get_device_name(0)} cap={torch.cuda.get_device_capability(0)}\"\n",
    "    log_line(f\"[HW] device={DEVICE} fp16={FP16} torch={torch.__version__} os={platform.system()} {platform.release()}{extra}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8f36e",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d31058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WORKDIR = Path.cwd()\n",
    "PENDING = WORKDIR / \"pending\"\n",
    "PROCESSING = WORKDIR / \"processing\"\n",
    "DONE = WORKDIR / \"done\"\n",
    "FAILED = WORKDIR / \"failed\"\n",
    "LOGFILE = WORKDIR / \"pipeline.log\"\n",
    "# Normalización de audio (WAV 16 kHz mono PCM16)\n",
    "NORMALIZE_AUDIO = True  # False si quieres desactivarlo temporalmente\n",
    "AUDIO_EXTS = {\".wav\", \".m4a\", \".mp3\", \".flac\", \".ogg\", \".oga\", \".ogx\", \".opus\", \".aac\", \".wma\", \".caf\", \".aiff\", \".aif\", \".aifc\", \".amr\", \".alaw\", \".ulaw\", \".ac3\", \".eac3\", \".dts\", \".mp4\", \".m4v\", \".mov\", \".mkv\", \".mka\", \".webm\", \".weba\", \".avi\", \".3gp\", \".3g2\", \".flv\", \".ts\", \".mp2\", \".mp1\"}\n",
    "# \"long\" = YYYYMMDD-HHMMSS-name, \"short\" = MMDDHHMM-name\n",
    "NAME_STYLE = \"long\"\n",
    "LANG = \"es\"\n",
    "BEAM_SIZE = 8\n",
    "TEMPERATURE = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f13b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL_PROMPT = (\n",
    "#     \"Transcripción fiel en español de un archivo de audio, cualquiera sea su contexto: conferencia, reunión, clase, entrevista, discurso, enseñanza, narración, conversación o grabación personal. \"\n",
    "#     \"Usar ortografía y gramática correctas, con buena puntuación, manteniendo la coherencia y fidelidad al contenido original. \"\n",
    "#     \"El estilo debe ser claro, estructurado y sin inventar información. \"\n",
    "#     \"Palabras clave frecuentes: reunión, conferencia, clase, enseñanza, discurso, entrevista, conversación, narración, audio, transcripción, documento, claridad, precisión, fidelidad, coherencia, comprensión, lenguaje, contexto.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b093d",
   "metadata": {},
   "source": [
    "Prompt IDMJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d826057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PROMPT = (\n",
    "    \"Transcripción fiel en español\"\n",
    "    \"Usar ortografía y gramática correctas, con buena puntuación, manteniendo la coherencia y fidelidad al mensaje original. \"\n",
    "    \"Corresponde a una enseñanza doctrinal de la Iglesia de Dios Ministerial de Jesucristo Internacional (IDMJI).\"\n",
    "    \"El estilo debe ser claro, estructurado y sin inventar información. \"\n",
    "    # \"Cuando se mencione un versículo bíblico, adicionalmente debe transcribirse con el siguiente formato entre corchetes, indicando libro, capítulo y versículos según la Biblia Reina-Valera 1960. Y se agrega justo despues de donde se menciona el versiculo. Ejemplo: [San Lucas 5:27-28]. \"\n",
    "    \"Palabras clave frecuentes: Iglesia, Doctrina, Enseñanza, Profecía, Biblia, Dios, Espíritu Santo.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc75afa",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69d8ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfc(s: str) -> str: return unicodedata.normalize(\"NFC\", s)\n",
    "def slugify(filename: str) -> str:\n",
    "    base = Path(filename).stem\n",
    "    base = unicodedata.normalize(\"NFC\", base).strip().casefold()\n",
    "    base = re.sub(r\"[.\\s]+\", \"-\", base)\n",
    "    base = re.sub(r\"[^\\w\\-.]+\", \"-\", base)\n",
    "    base = re.sub(r\"-{2,}\", \"-\", base).strip(\"-\")\n",
    "    return base or \"audio\"\n",
    "def ts_long() -> str: return datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "def ts_short() -> str: return datetime.datetime.now().strftime(\"%m%d%H%M\")\n",
    "def make_job_name(audio_path: Path) -> str:\n",
    "    ts = ts_long() if NAME_STYLE == \"long\" else ts_short()\n",
    "    model_tag = slugify(MODEL_NAME)\n",
    "    return f\"{ts}-{model_tag}-{slugify(audio_path.name)}\"\n",
    "def list_audios(pending_dir: Path):\n",
    "    def has_allowed_ext(p: Path) -> bool:\n",
    "        return any(ext.lower() in AUDIO_EXTS for ext in p.suffixes)\n",
    "    files = []\n",
    "    for p in pending_dir.iterdir():\n",
    "        if p.is_file():\n",
    "            if has_allowed_ext(p):\n",
    "                files.append(p)\n",
    "            else:\n",
    "                log_line(f\"[SKIP] {p.name} (Extensión no permitida)\")\n",
    "    return sorted(files, key=lambda p: p.stat().st_ctime)\n",
    "def log_line(msg: str):\n",
    "    LOGFILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with LOGFILE.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.datetime.now().isoformat()}  {msg}\\n\")\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a51455",
   "metadata": {},
   "source": [
    "RTF (Real-Time Factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cb68102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffprobe_duration(path: str | Path) -> float:\n",
    "    \"\"\"Duración en segundos usando ffprobe (preciso y rápido).\"\"\"\n",
    "    out = subprocess.check_output([\n",
    "        \"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
    "        \"-of\", \"default=noprint_wrappers=1:nokey=1\", str(path)\n",
    "    ], text=True).strip()\n",
    "    try:\n",
    "        return float(out)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def sha1_file(path: Path) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aff1bc",
   "metadata": {},
   "source": [
    "Normalización de audio (WAV 16 kHz mono PCM16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d09605bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_wav_16k(in_path: Path, out_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Convierte cualquier entrada (audio o video) a WAV PCM 16 kHz mono.\n",
    "    -vn fuerza a ignorar video; aresample=soxr mejora la calidad del remuestreo.\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\", \"-i\", str(in_path),\n",
    "        \"-vn\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-af\", \"aresample=resampler=soxr:precision=33\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-c:a\", \"pcm_s16le\",\n",
    "        str(out_path)\n",
    "    ]\n",
    "    # Capturamos stdout/stderr para que el notebook no se llene de logs de ffmpeg\n",
    "    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return out_path\n",
    "def prepareAudioforWhisper(src_path: Path, job_dir: Path, enabled: bool) -> Path:\n",
    "    \"\"\"\n",
    "    Si enabled=True → genera job_dir/'input.16k.wav' y lo devuelve.\n",
    "    Si enabled=False → devuelve el path original sin tocar.\n",
    "    \"\"\"\n",
    "    if not enabled:\n",
    "        return src_path\n",
    "    target = job_dir / \"input.16k.wav\"\n",
    "    return to_wav_16k(src_path, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537b9ab",
   "metadata": {},
   "source": [
    "Preparar carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19e57c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (PENDING, PROCESSING, DONE, FAILED): d.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea353c9",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a4ae358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Whisper 'base' en 'cpu' fp16=False…\n"
     ]
    }
   ],
   "source": [
    "log_line(f\"Cargando Whisper '{MODEL_NAME}' en '{DEVICE}' fp16={FP16}…\")\n",
    "model = whisper.load_model(MODEL_NAME, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76a9af",
   "metadata": {},
   "source": [
    "Bucle Procesador de Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "320891ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay audios en ./pending.\n"
     ]
    }
   ],
   "source": [
    "audios = list_audios(PENDING)\n",
    "if not audios:\n",
    "    print(\"No hay audios en ./pending.\")\n",
    "else:\n",
    "    total_jobs = 0\n",
    "    ok_jobs = 0\n",
    "    failed_jobs = 0\n",
    "    total_elapsed = datetime.timedelta()\n",
    "    total_audio_dur = 0.0\n",
    "    normalized_count = 0\n",
    "    rtfs = []\n",
    "\n",
    "    for audio_in in audios:\n",
    "        # --- crear carpeta de trabajo del job ---\n",
    "        job_name = make_job_name(audio_in)\n",
    "        job_dir = PROCESSING / job_name\n",
    "        job_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --- copiar el original a processing/<job> ---\n",
    "        audio_tmp = job_dir / audio_in.name\n",
    "        shutil.copy2(str(audio_in), str(audio_tmp))\n",
    "\n",
    "        # --- cronómetro: mide de principio a fin ---\n",
    "        start_wall = datetime.datetime.now()\n",
    "        start_perf = time.perf_counter()\n",
    "\n",
    "        # --- preparar fuente para Whisper (normalizar a 16k) con fallback ---\n",
    "        try:\n",
    "            src_for_whisper = prepareAudioforWhisper(audio_tmp, job_dir, enabled=NORMALIZE_AUDIO)\n",
    "            if src_for_whisper != audio_tmp:\n",
    "                log_line(f\"[AUDIO] Normalizado a 16k WAV: {src_for_whisper.name}\")\n",
    "            else:\n",
    "                log_line(f\"[AUDIO] Sin normalizar (usando original): {audio_tmp.name}\")\n",
    "            # Conteo de normalizados\n",
    "            if src_for_whisper != audio_tmp:\n",
    "                normalized_count += 1\n",
    "        except Exception as prep_err:\n",
    "            src_for_whisper = audio_tmp\n",
    "            log_line(f\"[AUDIO] Preparación falló, uso original: {audio_tmp.name} :: {prep_err}\")\n",
    "        log_line(f\"[START] {audio_in.name} -> {job_dir.name} [model={MODEL_NAME}, device={DEVICE}, fp16={FP16}]\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            # --- transcribir usando la fuente elegida ---\n",
    "            result = model.transcribe(\n",
    "                str(src_for_whisper),\n",
    "                language=LANG,\n",
    "                task=\"transcribe\",\n",
    "                temperature=TEMPERATURE,\n",
    "                beam_size=BEAM_SIZE,\n",
    "                patience=1.0,\n",
    "                condition_on_previous_text=True,\n",
    "                initial_prompt=INITIAL_PROMPT if INITIAL_PROMPT.strip() else None,\n",
    "                fp16=FP16\n",
    "            )\n",
    "\n",
    "            # --- limpiar texto ---\n",
    "            text = nfc(result.get(\"text\", \"\")).strip()\n",
    "            text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "            text = re.sub(r\"\\s+\\n\", \"\\n\", text).strip() + \"\\n\"\n",
    "\n",
    "            # --- escribir salida de texto ---\n",
    "            out_txt = job_dir / f\"{job_name}.txt\"\n",
    "            out_txt.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "            # --- métricas y meta ---\n",
    "            end_wall = datetime.datetime.now()\n",
    "            elapsed = datetime.timedelta(seconds=(time.perf_counter() - start_perf))\n",
    "\n",
    "            audio_used_path = Path(src_for_whisper)\n",
    "            audio_duration = ffprobe_duration(audio_used_path)\n",
    "            rtf = (elapsed.total_seconds() / audio_duration) if audio_duration > 0 else None\n",
    "\n",
    "            last_end = None\n",
    "            try:\n",
    "                segs = result.get(\"segments\")\n",
    "                if isinstance(segs, list) and segs:\n",
    "                    last_end = float(segs[-1].get(\"end\", 0.0))\n",
    "            except Exception:\n",
    "                last_end = None\n",
    "            coverage_ratio = (last_end / audio_duration) if (last_end and audio_duration) else None\n",
    "\n",
    "            meta = {\n",
    "                \"job_name\": job_name,\n",
    "                \"start_time\": start_wall.isoformat(),\n",
    "                \"end_time\": end_wall.isoformat(),\n",
    "                \"elapsed_sec\": round(elapsed.total_seconds(), 3),\n",
    "                \"audio_duration_sec\": round(audio_duration, 3),\n",
    "                \"rtf\": round(rtf, 3) if rtf is not None else None,\n",
    "                \"coverage_last_segment_end_sec\": round(last_end, 3) if last_end is not None else None,\n",
    "                \"coverage_ratio\": round(coverage_ratio, 4) if coverage_ratio is not None else None,\n",
    "\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"device\": DEVICE,\n",
    "                \"fp16\": FP16,\n",
    "                \"language\": LANG,\n",
    "                \"beam_size\": BEAM_SIZE,\n",
    "                \"temperature\": (list(TEMPERATURE) if isinstance(TEMPERATURE, tuple) else TEMPERATURE),\n",
    "                \"initial_prompt_len\": len(INITIAL_PROMPT.strip()),\n",
    "\n",
    "                \"normalized_16k\": bool(NORMALIZE_AUDIO and audio_used_path.name.endswith(\"input.16k.wav\")),\n",
    "                \"input_original_name\": audio_in.name,\n",
    "                \"input_original_sha1\": sha1_file(audio_in),\n",
    "                \"input_used_name\": audio_used_path.name,\n",
    "                \"input_used_sha1\": sha1_file(audio_used_path),\n",
    "                \"output_txt\": out_txt.name,\n",
    "\n",
    "                \"chars\": len(text),\n",
    "                \"words\": len(text.split()),\n",
    "                \"segments\": (len(result.get(\"segments\", [])) if isinstance(result.get(\"segments\"), list) else None),\n",
    "\n",
    "                \"whisper_version\": getattr(whisper, \"__version__\", \"git\"),\n",
    "                \"torch_version\": torch.__version__,\n",
    "                \"os\": f\"{platform.system()} {platform.release()}\",\n",
    "            }\n",
    "\n",
    "            # --- escritura atómica de meta.json ---\n",
    "            tmp_meta = job_dir / \"meta.json.tmp\"\n",
    "            tmp_meta.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "            tmp_meta.replace(job_dir / \"meta.json\")\n",
    "\n",
    "            # --- mover a done y limpiar pending ---\n",
    "            final_dir = DONE / job_name\n",
    "            shutil.move(str(job_dir), str(final_dir))\n",
    "            audio_in.unlink(missing_ok=False)\n",
    "\n",
    "            log_line(f\"[DONE]  {audio_in.name} -> {final_dir} (Duración: {elapsed}, RTF={meta['rtf']})\")\n",
    "\n",
    "            total_jobs += 1\n",
    "            ok_jobs += 1\n",
    "            total_elapsed += elapsed\n",
    "            # Solo acumula duración/RTF si la duración es válida (>0)\n",
    "            if audio_duration and audio_duration > 0:\n",
    "                total_audio_dur += audio_duration\n",
    "                if rtf is not None:\n",
    "                    rtfs.append(rtf)\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_jobs += 1\n",
    "            total_jobs += 1\n",
    "            tb = traceback.format_exc()\n",
    "            log_line(f\"[FAIL]  {audio_in.name} :: {e}\")\n",
    "            (job_dir / \"error.log\").write_text(tb, encoding=\"utf-8\")\n",
    "            failed_dir = (WORKDIR / \"failed\" / job_name)\n",
    "            failed_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(str(job_dir), str(failed_dir))\n",
    "            continue\n",
    "\n",
    "    print(\"\\n===== 📊 Informe del pipeline =====\")\n",
    "    print(f\"Total procesados: {total_jobs}\")\n",
    "    print(f\"  ✅ Exitosos: {ok_jobs}\")\n",
    "    print(f\"  ❌ Fallidos: {failed_jobs}\")\n",
    "\n",
    "    if total_jobs > 0:\n",
    "        print(f\"Normalizados: {normalized_count}/{total_jobs} \"\n",
    "            f\"({(100.0*normalized_count/total_jobs):.1f}%)\")\n",
    "\n",
    "    if ok_jobs > 0:\n",
    "        # Promedios solo con éxitos y con duraciones válidas\n",
    "        if total_audio_dur > 0:\n",
    "            avg_audio = total_audio_dur / ok_jobs\n",
    "            print(f\"Duración promedio de audio: {avg_audio:.1f} s\")\n",
    "        else:\n",
    "            print(\"Duración promedio de audio: N/A\")\n",
    "\n",
    "        avg_elapsed = total_elapsed / ok_jobs\n",
    "        print(f\"Tiempo promedio de ejecución: {avg_elapsed}\")\n",
    "\n",
    "        if rtfs:\n",
    "            avg_rtf = sum(rtfs) / len(rtfs)\n",
    "            print(f\"RTF promedio: {avg_rtf:.3f}\")\n",
    "        else:\n",
    "            print(\"RTF promedio: N/A\")\n",
    "    else:\n",
    "        print(\"No hubo jobs exitosos.\")\n",
    "\n",
    "    print(\"===================================\")\n",
    "    print(\"\\nProceso terminado. Revisa ./done para los finalizados y ./pending para los no procesados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
